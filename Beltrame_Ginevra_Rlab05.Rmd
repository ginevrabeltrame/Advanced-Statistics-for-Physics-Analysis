---
title: "Beltrame_Ginevra_Rlab05"
output: html_document
date: "2024-05-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 1

Given the following un-normalized posterior distribution

$$
g(\theta | x) \propto \frac{e^{-\frac{(\theta + 3)^2}{2}} + e^{-\frac{(\theta - 3)^2}{2}}}{2} 
$$

-   draw a Markov Chain from the posterior distribution using a Metropolis-Hastings algorithm

-   use a Norm(0, 1) as random-walk candidate density

-   plot the sampled distribution

-   analyze the chain with the CODA package and plot the chain autocorrelation

-   try to use different burn-in cycles and thinning and plot the corresponding posterior distribution and the chain autocorrelation function. What are the best parameters?

### Solution

```{r}
metropolis.1dim <- function(func, theta.init, n.sample, sigma) {
  theta.cur <- theta.init
  func.Cur <- func(theta.cur)
  func.Samp <- matrix(data=NA , nrow=n.sample , ncol=2+1)
  n.accept <- 0
  rate.accept <- 0.0
  for (n in 1:n.sample) {
    theta.prop <- theta.cur + rnorm(n=1, mean=0, sd=sigma) # I use Norm(0,1) as random-walk candidate density
    func.Prop <- func(theta.prop)
    logMR <- func.Prop - func.Cur # Log10 of the Metropolis ratio
    if ( logMR>=0 || logMR>log10(runif(1)) ) {
      theta.cur <- theta.prop
      func.Cur <- func.Prop
      n.accept <- n.accept + 1
    }
    func.Samp[n, 1] <- func.Cur
    func.Samp[n, 2] <- theta.cur
  }
  return(func.Samp)
}

# Test function
testfunc <- function(theta) {
  return((exp(-(theta+3)^2/2) + exp(-(theta-3)^2/2))/2)
}

# interface for the metropolis function, gets the log10 of test function
testfunc.metropolis <- function(theta) {
  return(log10(testfunc(theta)))
}

# Parameters
theta.init <- 0
sigma <- 1
n.sample <- 10^6
set.seed(123456)
chain <- metropolis.1dim(func=testfunc.metropolis, theta.init=theta.init, n.sample=n.sample, sigma=sigma)

# Plot
par(mfrow=c(1,1), mgp=c(2,0.8,0), mar=c(3.5,3.5,1,1), oma=0.1*c(1,1,1,1))
x <- seq(-10, 10, length.out=10^4)
y <- testfunc(x)
ymax <- 1.05 * max(y)
plot(x, y, type="n", yaxs="i", xlim=c(-10,10), ylim=c(0, 1.25*max(y)), xlab=expression(theta), ylab=expression(paste('f(', theta, ')', sep='')))

sa <- which(chain[,2]>=min(x) & chain[,2]<=max(x))
hist <- hist(chain[sa,2], breaks=seq(from=min(x), to=max(x), length.out=100), plot=FALSE)
Zhist <- sum(hist$counts)*diff(range(hist$breaks))/(length(hist$counts))
Zfunc <- sum(y*(x[2]-x[1]))
lines(hist$breaks, c(hist$counts*Zfunc/Zhist, 0), col='navy', type="s", lwd=2, lty=3)
lines(x, y, col='firebrick3', lwd=1, lty=1)


n.sample <- 50000
chain <- metropolis.1dim(func=testfunc.metropolis, theta.init = theta.init, n.sample=n.sample, sigma=sigma)
sa <- which(chain[,2]>=min(x) & chain[,2]<=max(x))
hist <- hist(chain[sa,2], breaks=seq(from=min(x), to=max(x), length.out=100), plot=FALSE)
Zhist <- sum(hist$counts)*diff(range(hist$breaks))/(length(hist$counts))
lines(hist$breaks, c(hist$counts*Zfunc/Zhist ,0), col='green', type="s", lwd=2, lty=3)

leg.labels = c('analytical', 'MCMC, 10^6 samples', 'MCMC, 50000 samples')
leg.ltype = c(1, 3, 3)
leg.colors = c('firebrick3','navy', 'green')
legend("topleft", inset =.005, bty='n', legend=leg.labels, lty=leg.ltype, col=leg.colors, lwd = 2)
```

From the previous plot it can be inferred that both the MCMC samplings yield a posterior distribution which adheres very well to the analytical curve.

```{r}
library(coda)
par(mfrow=c(2,2), mgp=c(2,0.8,0), mar=c(3.5,3.5,1,1), oma=0.1*c(1,1,1,1))
colors <- c('navy', 'red', 'darkgreen', 'orange')

for (i in seq(1,4)){
  chain1 <- metropolis.1dim(func=testfunc.metropolis, theta.init=0, n.sample=10^6, sigma=i*0.5)
  c.chain1 <- as.mcmc(chain1[,2])
  my.lags = seq(0,500,10)
  y1 <- autocorr(c.chain1, lags=my.lags)
  
  plot(my.lags , y1, ylim=c(0,1), pch=16, col=colors[i], xlab='lag', ylab='ACF', cex=1.3)
  text(400,0.95, paste('sigma=', i*0.5))
  text(350,0.85, sprintf("effective size: %.1f", effectiveSize(c.chain1)))

}

```

From the previous plot it is apparent that the best choice of the $\sigma$ parameter is 2, yielding a sample where the Auto Correlation Fraction (ACF) becomes negligible after the first 100 lags.

```{r, fig.width=10, fig.height=6}
metropolis.burn_thin <- function(func, theta.init, n.sample, sigma, burn_in, thin) {
  theta.cur <- theta.init
  func.Cur <- func(theta.cur)
  func.Samp <- matrix(data=NA , nrow=n.sample , ncol=2+1)
  n.accept <- 0
  rate.accept <- 0.0
  for (n in 1:n.sample) {
    theta.prop <- theta.cur + rnorm(n=1, mean=0, sd=sigma) # I use Norm(0,1) as random-walk candidate density
    func.Prop <- func(theta.prop)
    logMR <- func.Prop - func.Cur # Log10 of the Metropolis ratio
    if ( logMR>=0 || logMR>log10(runif(1)) ) {
      theta.cur <- theta.prop
      func.Cur <- func.Prop
      n.accept <- n.accept + 1
    }
    func.Samp[n, 1] <- func.Cur
    func.Samp[n, 2] <- theta.cur
  }
  
  func.Samp <- func.Samp[(burn_in + 1):n.sample,]  # Remove burn-in samples
  func.Samp <- func.Samp[seq(1, nrow(func.Samp), by = thin),]  # Apply thinning
  
  return(func.Samp)
}

# Test function
testfunc <- function(theta) {
  return((exp(-(theta+3)^2/2) + exp(-(theta-3)^2/2))/2)
}

# interface for the metropolis function, gets the log10 of test function
testfunc.metropolis <- function(theta) {
  return(log10(testfunc(theta)))
}

# Plot
library(coda)
par(mfrow=c(2,2), mgp=c(2,0.8,0), mar=c(3.5,3.5,1,1), oma=0.1*c(1,1,1,1))
n.sample <- 10^6
burn_ins <- n.sample*c(0.05, 0.1, 0.15, 0.25, 0.25, 0.025, 0.05, 0.1, 0.15, 0.25)
thins <- c(10,25,50,100,200,10,25,50,100,200)
colors <- c('blue', 'red', 'darkgreen', 'orange', 'violet')

for (j in seq(1,2)){
  x <- seq(-10, 10, length.out=10^4)
  y <- testfunc(x)
  ymax <- 1.05 * max(y)
  Zfunc <- sum(y*(x[2]-x[1]))
  my.lags = seq(0,500,5)
  plot(x, y, type="n", yaxs="i", xlim=c(-10,10), ylim=c(0, 1.25*max(y)), xlab=expression(theta), ylab=expression(paste('f(', theta, ')', sep='')), main=paste("Sigma =", j))
  
  for (i in seq(1,5)){
  chain1 <- metropolis.burn_thin(func=testfunc.metropolis, theta.init=0, n.sample=n.sample, sigma=j, burn_in=burn_ins[i+5*(j-1)], thin=thins[i+5*(j-1)])
  c.chain1 <- as.mcmc(chain1[,2])
  
  y1 <- autocorr(c.chain1, lags=my.lags)
  
  sa <- which(chain1[,2]>=min(x) & chain1[,2]<=max(x))
  hist <- hist(chain1[sa,2], breaks=seq(from=min(x), to=max(x), length.out=100), plot=FALSE)
  Zhist <- sum(hist$counts)*diff(range(hist$breaks))/(length(hist$counts))
  
  lines(hist$breaks, c(hist$counts*Zfunc/Zhist ,0), col=colors[i], type="s", lwd=2, lty=1)
  }
  
  leg.labels = c(paste('burn-in:', burn_ins[1+5*(j-1)], '\nthin:', thins[1+5*(j-1)]), paste('burn-in:', burn_ins[2+5*(j-1)], '\nthin:', thins[2+5*(j-1)]), paste('burn-in:', burn_ins[3+5*(j-1)], '\nthin:', thins[3+5*(j-1)]), paste('burn-in:', burn_ins[4+5*(j-1)], '\nthin:', thins[4+5*(j-1)]), paste('burn-in:', burn_ins[5+5*(j-1)], '\nthin:', thins[5+5*(j-1)]))
  
  plot(x, y, xlim=c(0,500), ylim=c(0,1), xlab='lag', ylab='ACF', main=paste("Sigma =", j), col='white')
  
  for (i in seq(1,5)){
    chain1 <- metropolis.burn_thin(func=testfunc.metropolis, theta.init=0, n.sample=n.sample, sigma=j, burn_in=burn_ins[i+5*(j-1)], thin=thins[i+5*(j-1)])
    c.chain1 <- as.mcmc(chain1[,2])
    
    y1 <- autocorr(c.chain1, lags=my.lags)
    lines(my.lags , y1, col=colors[i], lty=1, lwd=1.5)
  }
  legend("topright", legend=leg.labels, col=colors, lty=1, lwd=1.5)
}

```

It is evident that, by choosing $\sigma = 2$, all the possible choices of the burn-in and thinning parameters are almost equivalent in terms of the autocorrelation curve. The best choice is then the less computationally expensive.

## Exercise 2

-   A set of measured data should follow, according to the physics model applied to them, a linear behavior. Data are the following:

|     |        |        |         |         |         |         |         |         |
|-----|--------|--------|---------|---------|---------|---------|---------|---------|
| Y   | -7.821 | -1.494 | -15.444 | -10.807 | -13.735 | -14.442 | -15.892 | -18.326 |
| X   | 5      | 6      | 7       | 8       | 9       | 10      | 11      | 12      |

-   perform a simple linear regression model running a Markov Chain Monte Carlo with JAGS, assuming that data follow the model: $Z[i] = a + b * X[i]$;

-   and the likelihood of the measured data follow a Gaussian likelihood distribution: $Y[i] \approx dnorm(Z[i], c)$

-   you can constrain the parameter $a$, $b$ and $c$ to the following intervals: $a ∈ [1, 10]$, $b ∈ [−1, 3]$ and $c ∈ [0.034, 4]$

-   run JAGS experimenting with the burn-in and number of iterations of the chain. Plot the evolution of the chains and the posterior distributions of $a$ and $b$. Compute the 95% credibility interval for the parameters.

-   using the obtained posterior distributions, compute the posterior distribution of $σ = \frac{1}{\sqrt{c}}$

### Solution

```{r, fig.width=10, fig.height=10}
library(rjags)

x <- seq(5,12,1)
y <- c(-7.821, -1.494, -15.444, -10.807, -13.735, -14.442, -15.892, -18.326)
data <- list(X = x, Y = y, N = length(y))

# Write the JAGS model to a file
model_string <- "
model {
  # Priors for the coefficients and variance
  a ~ dunif(1, 10)
  b ~ dunif(-1, 3)
  c ~ dunif(0.034, 4)
  
  # Likelihood
  for (i in 1:N) {
    Y[i] ~ dnorm(Z[i], c)
    Z[i] <- a + b * X[i]
  }
}
"
writeLines(model_string, "linear_model.jags")

# Parameters to monitor
params <- c("a", "b", "c")

# Initialize JAGS model
jags.model <- jags.model(file = "linear_model.jags", data = data)

# Burn-in
burn_in <- 500
update(jags.model, n.iter = burn_in)

# Sample from the posterior
n_iterations_chain <- 5000
samples <- coda.samples(model = jags.model, variable.names = params, n.iter = n_iterations_chain)

# Summary of the samples
summary(samples)
```

```{r, fig.width=10, fig.height=10}
# Plot the samples
par(mfrow=c(3,2))

traceplot(samples[, 'a'], col='navy', main='Trace of a')
densplot(samples[, 'a'], col='navy', main='Density of a')
summary_a <- summary(samples[, 'a'])
cred_interval_a <- summary_a$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_a, col = "blue")
text(3.2, 1.35, paste('Credibility interval 95% \n[', signif(cred_interval_a[1], 3), ',', signif(cred_interval_a[2], 3), ']'), col='blue')

traceplot(samples[, 'b'], col='darkred', main='Trace of b')
densplot(samples[, 'b'], col='darkred', main='Density of b')
summary_b <- summary(samples[, 'b'])
cred_interval_b <- summary_b$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_b, col = "red")
text(-0.73, 12.5, paste('Credibility interval 95% \n[', signif(cred_interval_b[1], 3), ',', signif(cred_interval_b[2], 3), ']'), col='red')

traceplot(samples[, 'c'], col='darkgreen', main='Trace of c')
densplot(samples[, 'c'], col='darkgreen', main='Density of c')
summary_c <- summary(samples[, 'c'])
cred_interval_c <- summary_c$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_c, col = "green")
text(0.08, 65, paste('Credibility interval 95% \n[', signif(cred_interval_c[1], 3), ',', signif(cred_interval_c[2], 3), ']'), col='darkgreen')

par(mfrow=c(1,1))
```

```{r, fig.width=10, fig.height=6}
samples_matrix <- as.matrix(samples)
sigma_samples <- 1 / sqrt(samples_matrix[, "c"])

cred_interval_sigma <- quantile(sigma_samples, probs = c(0.025, 0.975))

hist(sigma_samples, breaks = 50, main = expression(paste("Posterior distribution of ", sigma)),
     xlab = expression(sigma), col = "lightblue", border = "black")
abline(v = cred_interval_sigma, col = "red", lwd = 2, lty = 2)
text(3.5, 250, paste('Credibility interval 95% \n[', signif(cred_interval_sigma[1], 3), ',', signif(cred_interval_sigma[2], 3), ']'), col='red')

```

## Exercise 3

-   suppose we observe the following values $x$ = 2.06, 5.56, 7.93, 6.56, 205

-   and we assume that the data come from a gaussian distribution with unknown mean $m$ and variance $s^2$

-   build a simple JAGS model and run a Markov Chain Monte Carlo to obtain the posterior distribution of the mean and variance.

-   Assume uniform prior distributions for the parameters, $m \approx dunif(-10, 10)$ and $s \approx dunif(0,50)$.

-   compute also the posterior distribution for $\frac{m}{s}$

### Solution

```{r, fig.width=10, fig.height=10}

x <- c(2.06, 5.56, 7.93, 6.56, 2.05)

model_string <- "
model {
  for (i in 1:5) {
    X[i] ~ dnorm(mu, 1/s2);
  }
  mu ~ dunif(-10, 10);
  sigma ~ dunif(0,50);
  s2 <- sigma^2
  Y ~ dnorm(mu, 1/s2);
}
"
writeLines(model_string, "normal_model.bug")

# Specify the Generative Model with BUGS
model <- "normal_model.bug"

# Our data for the model
data <- NULL
data$X <- x # Set of observations

# Create the model and pass the parameters
jm <- jags.model(model, data)

# Update the Markov chain (Burn-in)
burn_in <- 5000
update(jm, n.iter = burn_in)

# Sample from the posterior
n_iterations_chain <- 5000
samples <- coda.samples(model = jm, variable.names = c("mu", "s2", "Y"), n.iter = n_iterations_chain)

# Summary of the samples
summary(samples)
```

```{r, fig.width=10, fig.height=10}
# Plot the samples
par(mfrow=c(3,2))

traceplot(samples[, 'mu'], col='navy', main='Trace of m')
densplot(samples[, 'mu'], col='navy', main='Density of m')
summary_a <- summary(samples[, 'mu'])
cred_interval_a <- summary_a$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_a, col = "blue")
text(-3, 0.08, paste('Credibility interval 95% \n[', signif(cred_interval_a[1], 3), ',', signif(cred_interval_a[2], 3), ']'), col='blue')

traceplot(samples[, 's2'], col='darkred', main='Trace of s^2')
densplot(samples[, 's2'], col='darkred', main='Density of s^2')
summary_b <- summary(samples[, 's2'])
cred_interval_b <- summary_b$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_b, col = "red")
text(250, 0.01, paste('Credibility interval 95% \n[', signif(cred_interval_b[1], 3), ',', signif(cred_interval_b[2], 3), ']'), col='red')

traceplot(samples[, 'Y'], col='darkgreen', main='Trace of Y')
densplot(samples[, 'Y'], col='darkgreen', main='Density of Y')
summary_c <- summary(samples[, 'Y'])
cred_interval_c <- summary_c$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_c, col = "green")
text(-25, 0.04, paste('Credibility interval 95% \n[', signif(cred_interval_c[1], 3), ',', signif(cred_interval_c[2], 3), ']'), col='darkgreen')

par(mfrow=c(1,1))

```

```{r}
samples_matrix <- as.matrix(samples)
ms_samples <- samples_matrix[, "mu"] / sqrt(samples_matrix[, "s2"])

cred_interval_ms <- quantile(ms_samples, probs = c(0.025, 0.975))

hist(ms_samples, breaks = 50, main = paste("Posterior distribution of m/s"), xlab = "m/s", col = "lightblue", border = "black")
abline(v = cred_interval_ms, col = "red", lwd = 2, lty = 2)
text(-0.05, 220, paste('Credibility interval 95% \n[', signif(cred_interval_ms[1], 3), ',', signif(cred_interval_ms[2], 3), ']'), col='red')

```

## Exercise 4

The data set that Edwin Hubble used to show that galaxies are moving either away or towards us are given in the following table:

```         
| D   | 0.0032 | 0.0034 | 0.214 | 0.263 | 0.275 |
| V   | 170    | 290    | -130  | -70   | -185  |
|-----|--------|--------|-------|-------|-------|
| D   | 0.275  | 0.45   | 0.5   | 0.5   | 0.63  |
| V   | -220   | 200    | 290   | 270   | 200   |
|-----|--------|--------|-------|-------|-------|
| D   | 0.8    | 0.9    | 0.9   | 0.9   | 0.9   |
| V   | 920    | 450    | 500   | 500   | 960   |
|-----|--------|--------|-------|-------|-------|
| D   | 2      | 2      | 2     | 2     |       |
| V   | 500    | 850    | 800   | 1090  |       |
```

-   Using this data set define a JAGS model to fit data with the following: $V[i] \approx dnorm(b * D[i], c)$, where V represent the velocity in units of km/s, D is the observed distance (in units of parsec), and b and c are two parameters of the model. Assume whatever prior distribution you think is appropriate.

-   Plot the evolution of the chains, the posterior distribution of the parameters and the 95% credibility interval.

### Solution

```{r, fig.width=10, fig.height=10}
D <- c(0.0032, 0.0034, 0.214, 0.263, 0.275, 0.275, 0.45, 0.5, 0.5, 0.63, 0.8, 0.9, 0.9, 0.9, 0.9, 2, 2, 2, 2)
V <- c(170, 290, -130, -70, -185, -220, 200, 290, 270, 200, 920, 450, 500, 500, 960, 500, 850, 800, 1090)

data <- list(X = D, Y = V, N = length(V))

# Write the JAGS model to a file
model_string <- "
model {
  b ~ dunif(0, 2000)
  c ~ dunif(-220, 1090)
  
  for (i in 1:N) {
    Y[i] ~ dnorm(Z[i], c)
    Z[i] <- b * X[i]
  }
}
"
writeLines(model_string, "Hubble_model.bug")

# Parameters to monitor
params <- c("b", "c")

# Initialize JAGS model
jags.model <- jags.model(file = "Hubble_model.bug", data = data)

# Burn-in
burn_in <- 500
update(jags.model, n.iter = burn_in)

# Sample from the posterior
n_iterations_chain <- 5000
samples <- coda.samples(model = jags.model, variable.names = params, n.iter = n_iterations_chain)

# Summary of the samples
summary(samples)
```

```{r, fig.width=10, fig.height=6}

par(mfrow=c(2,2))

traceplot(samples[, 'b'], col='darkred', main='Trace of b')
densplot(samples[, 'b'], col='darkred', main='Density of b')
summary_b <- summary(samples[, 'b'])
cred_interval_b <- summary_b$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_b, col = "red")
text(650, 0.005, paste('Credibility interval 95% \n[', signif(cred_interval_b[1], 1), ',', signif(cred_interval_b[2], 1), ']'), col='red')

traceplot(samples[, 'c'], col='darkgreen', main='Trace of c')
densplot(samples[, 'c'], col='darkgreen', main='Density of c')
summary_c <- summary(samples[, 'c'])
cred_interval_c <- summary_c$quantiles[c("2.5%", "97.5%")]
abline(v = cred_interval_c, col = "green")
text(3e-05, 40000, paste('Credibility interval 95% \n[', signif(cred_interval_c[1], 2), ',', signif(cred_interval_c[2], 2), ']'), col='darkgreen')

par(mfrow=c(1,1))

```
